{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Google Tunix Hackathon - Multi-Domain Reasoning Training\n",
                "\n",
                "**Strategy**: OpenThoughts + GSM8K with GRPO\n",
                "**Model**: Gemma 3 1B IT + LoRA\n",
                "\n",
                "## Key Improvements\n",
                "1. Multi-reward system (format + accuracy)\n",
                "2. Optimized hyperparameters for 9-hour TPU session\n",
                "3. Enhanced evaluation metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
                "\n",
                "!pip install -q kagglehub ipywidgets\n",
                "!pip install -q tensorflow tensorflow_datasets tensorboardX\n",
                "!pip install -q transformers grain datasets\n",
                "!pip install \"google-tunix[prod]==0.1.3\"\n",
                "!pip uninstall -q -y flax\n",
                "!pip install -U flax"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import functools\n",
                "import gc\n",
                "import os\n",
                "import re\n",
                "from pprint import pprint\n",
                "\n",
                "from flax import nnx\n",
                "import grain\n",
                "import humanize\n",
                "import jax\n",
                "import jax.numpy as jnp\n",
                "import kagglehub\n",
                "import optax\n",
                "from orbax import checkpoint as ocp\n",
                "from pathlib import Path\n",
                "import qwix\n",
                "from tqdm.auto import tqdm\n",
                "from tunix.generate import sampler as sampler_lib\n",
                "from tunix.models.gemma3 import params, model\n",
                "from tunix.rl import rl_cluster as rl_cluster_lib\n",
                "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
                "from tunix.rl.rollout import base_rollout\n",
                "from tunix.sft import metrics_logger\n",
                "from datasets import load_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ====== Model & LoRA ======\n",
                "LORA_RANK = 32\n",
                "LORA_ALPHA = 32.0\n",
                "\n",
                "# ====== Sharding ======\n",
                "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
                "\n",
                "# ====== GRPO ======\n",
                "MAX_PROMPT_LENGTH = 256\n",
                "TOTAL_GENERATION_STEPS = 512\n",
                "TEMPERATURE = 0.9\n",
                "TOP_P = 1.0\n",
                "TOP_K = 50\n",
                "NUM_GENERATIONS = 4\n",
                "NUM_ITERATIONS = 1\n",
                "BETA = 0.08\n",
                "EPSILON = 0.2\n",
                "\n",
                "# ====== Training ======\n",
                "TRAIN_MICRO_BATCH_SIZE = 2\n",
                "NUM_BATCHES = 3738\n",
                "NUM_TEST_BATCHES = 100\n",
                "EVAL_EVERY_N_STEPS = 10\n",
                "NUM_EPOCHS = 1\n",
                "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * NUM_EPOCHS)\n",
                "\n",
                "# ====== Optimizer ======\n",
                "LEARNING_RATE = 3e-6\n",
                "B1 = 0.9\n",
                "B2 = 0.99\n",
                "WEIGHT_DECAY = 0.1\n",
                "WARMUP_STEPS = int(0.1 * MAX_STEPS)\n",
                "MAX_GRAD_NORM = 0.1\n",
                "\n",
                "# ====== Checkpointing ======\n",
                "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
                "CKPT_DIR = \"/tmp/content/ckpts/\"\n",
                "SAVE_INTERVAL_STEPS = 500\n",
                "MAX_TO_KEEP = 4"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset, concatenate_datasets\n",
                "import random\n",
                "\n",
                "# Reasoning format tags\n",
                "reasoning_start = \"<reasoning>\"\n",
                "reasoning_end = \"</reasoning>\"\n",
                "solution_start = \"<answer>\"\n",
                "solution_end = \"</answer>\"\n",
                "\n",
                "SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
                "provide your reasoning. Place it between {reasoning_start} and \\\n",
                "{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
                "value) between {solution_start} and {solution_end}.\"\"\"\n",
                "\n",
                "TEMPLATE = \"\"\"<start_of_turn>user\n",
                "{system_prompt}\n",
                "\n",
                "{question}<end_of_turn>\n",
                "<start_of_turn>model\"\"\"\n",
                "\n",
                "def extract_hash_answer(text):\n",
                "    \"\"\"Extract answer from GSM8K format (#### answer).\"\"\"\n",
                "    if \"####\" not in text:\n",
                "        return None\n",
                "    return text.split(\"####\")[1].strip()\n",
                "\n",
                "def format_gsm8k_for_grpo(example):\n",
                "    \"\"\"Format GSM8K data for GRPO phase.\"\"\"\n",
                "    return {\n",
                "        'prompts': TEMPLATE.format(\n",
                "            system_prompt=SYSTEM_PROMPT,\n",
                "            question=example['question']\n",
                "        ),\n",
                "        'question': example['question'],\n",
                "        'answer': extract_hash_answer(example['answer'])\n",
                "    }\n",
                "\n",
                "print(\"Loading GSM8K for GRPO...\")\n",
                "grpo_dataset = load_dataset(\n",
                "    \"gsm8k\",\n",
                "    \"main\",\n",
                "    split=\"train\"\n",
                ")\n",
                "grpo_dataset = grpo_dataset.map(format_gsm8k_for_grpo, remove_columns=grpo_dataset.column_names)\n",
                "train_dataset = grpo_dataset.select(range(min(len(grpo_dataset), NUM_BATCHES * TRAIN_MICRO_BATCH_SIZE)))\n",
                "\n",
                "print(f\"GRPO dataset size: {len(train_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import re\n",
                "\n",
                "# RegEx for format matching\n",
                "match_format = re.compile(\n",
                "    rf\"^[\\s]{{0,}}\"\n",
                "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n",
                "    rf\"{solution_start}(.+?){solution_end}\"\n",
                "    rf\"[\\s]{{0,}}$\",\n",
                "    flags=re.MULTILINE | re.DOTALL,\n",
                ")\n",
                "\n",
                "match_numbers = re.compile(\n",
                "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
                ")\n",
                "\n",
                "def match_format_exactly(prompts, completions, **kwargs):\n",
                "    \"\"\"Reward if format matches exactly.\"\"\"\n",
                "    return [\n",
                "        0 if match_format.search(response) is None else 3.0\n",
                "        for response in completions\n",
                "    ]\n",
                "\n",
                "def match_format_approximately(prompts, completions, **kwargs):\n",
                "    \"\"\"Reward if format matches partially.\"\"\"\n",
                "    scores = []\n",
                "    for completion in completions:\n",
                "        score = 0\n",
                "        response = completion\n",
                "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
                "        score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n",
                "        score += 0.5 if response.count(solution_start) == 1 else -0.5\n",
                "        score += 0.5 if response.count(solution_end) == 1 else -0.5\n",
                "        scores.append(score)\n",
                "    return scores\n",
                "\n",
                "def check_answer(prompts, completions, answer, **kwargs):\n",
                "    \"\"\"Reward if the answer is correct.\"\"\"\n",
                "    responses = completions\n",
                "    extracted_responses = [\n",
                "        guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
                "        for r in responses\n",
                "    ]\n",
                "    scores = []\n",
                "    for guess, true_answer in zip(extracted_responses, answer):\n",
                "        score = 0\n",
                "        if guess is None:\n",
                "            scores.append(0)\n",
                "            continue\n",
                "        if guess == true_answer:\n",
                "            score += 3.0\n",
                "        elif guess.strip() == true_answer.strip():\n",
                "            score += 1.5\n",
                "        else:\n",
                "            try:\n",
                "                ratio = float(guess) / float(true_answer)\n",
                "                if ratio >= 0.9 and ratio <= 1.1:\n",
                "                    score += 0.5\n",
                "                elif ratio >= 0.8 and ratio <= 1.2:\n",
                "                    score += 0.25\n",
                "                else:\n",
                "                    score -= 1.0\n",
                "            except:\n",
                "                score -= 0.5\n",
                "        scores.append(score)\n",
                "    return scores\n",
                "\n",
                "def check_numbers(prompts, completions, answer, **kwargs):\n",
                "    \"\"\"Extract numbers and check if correct.\"\"\"\n",
                "    responses = completions\n",
                "    extracted_responses = [\n",
                "        guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
                "        for r in responses\n",
                "    ]\n",
                "    scores = []\n",
                "    for guess, true_answer in zip(extracted_responses, answer):\n",
                "        if guess is None:\n",
                "            scores.append(0)\n",
                "            continue\n",
                "        try:\n",
                "            true_answer_num = float(true_answer.strip())\n",
                "            guess_num = float(guess.strip())\n",
                "            scores.append(1.5 if guess_num == true_answer_num else 0.0)\n",
                "        except:\n",
                "            scores.append(0)\n",
                "            continue\n",
                "    return scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Model\n",
                "!rm -rf {INTERMEDIATE_CKPT_DIR}/*\n",
                "!rm -rf {CKPT_DIR}/*\n",
                "\n",
                "MODEL_CP_PATH = params.GEMMA3_1B_IT\n",
                "mesh = jax.make_mesh(*MESH)\n",
                "config = model.ModelConfig.gemma3_1b()\n",
                "gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
                "tokenizer = params.create_tokenizer()\n",
                "\n",
                "checkpointer = ocp.StandardCheckpointer()\n",
                "_, state = nnx.split(gemma)\n",
                "checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
                "checkpointer.wait_until_finished()\n",
                "del gemma, state\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_gemma_ref_model(ckpt_path):\n",
                "    mesh = jax.make_mesh(*MESH)\n",
                "    model_config = model.ModelConfig.gemma3_1b()\n",
                "    abs_gemma: nnx.Module = nnx.eval_shape(\n",
                "        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
                "    )\n",
                "    abs_state = nnx.state(abs_gemma)\n",
                "    abs_state = jax.tree.map(\n",
                "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
                "        abs_state,\n",
                "        nnx.get_named_sharding(abs_state, mesh),\n",
                "    )\n",
                "    checkpointer = ocp.StandardCheckpointer()\n",
                "    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
                "    graph_def, _ = nnx.split(abs_gemma)\n",
                "    gemma = nnx.merge(graph_def, restored_params)\n",
                "    return gemma, mesh, model_config\n",
                "\n",
                "def get_lora_model(base_model, mesh):\n",
                "    lora_provider = qwix.LoraProvider(\n",
                "        module_path=(\n",
                "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
                "            \".*attn_vec_einsum\"\n",
                "        ),\n",
                "        rank=LORA_RANK,\n",
                "        alpha=LORA_ALPHA,\n",
                "    )\n",
                "    model_input = base_model.get_model_input()\n",
                "    lora_model = qwix.apply_lora_to_model(\n",
                "        base_model, lora_provider, **model_input\n",
                "    )\n",
                "    with mesh:\n",
                "        state = nnx.state(lora_model)\n",
                "        pspecs = nnx.get_partition_spec(state)\n",
                "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
                "        nnx.update(lora_model, sharded_state)\n",
                "    return lora_model\n",
                "\n",
                "ref_model, mesh, model_config = get_gemma_ref_model(\n",
                "    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n",
                ")\n",
                "lora_policy = get_lora_model(ref_model, mesh=mesh)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optimizer\n",
                "optimizer = optax.adamw(\n",
                "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
                "        init_value=0.0,\n",
                "        peak_value=LEARNING_RATE,\n",
                "        warmup_steps=WARMUP_STEPS,\n",
                "        decay_steps=MAX_STEPS,\n",
                "        end_value=0.0,\n",
                "    ),\n",
                "    b1=B1,\n",
                "    b2=B2,\n",
                "    weight_decay=WEIGHT_DECAY,\n",
                ")\n",
                "optimizer = optax.chain(\n",
                "    optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n",
                "    optimizer,\n",
                ")\n",
                "\n",
                "# GRPO Config\n",
                "cluster_config = rl_cluster_lib.ClusterConfig(\n",
                "    role_to_mesh={\n",
                "        rl_cluster_lib.Role.ACTOR: mesh,\n",
                "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
                "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
                "    },\n",
                "    rollout_engine='vanilla',\n",
                "    offload_to_cpu=False,\n",
                "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
                "        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        metrics_logging_options=None,\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=ocp.CheckpointManagerOptions(\n            save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n        ),\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1,106],\n    ),\n)\n",
                "\n",
                "grpo_config = GRPOConfig(\n",
                "    num_generations=NUM_GENERATIONS,\n",
                "    num_iterations=NUM_ITERATIONS,\n",
                "    beta=BETA,\n    epsilon=EPSILON,\n",
                ")\n",
                "\n",
                "# Trainer\n",
                "rl_cluster = rl_cluster_lib.RLCluster(\n",
                "    actor=lora_policy,\n",
                "    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n",
                ")\n",
                "\n",
                "grpo_trainer = GRPOLearner(\n",
                "    rl_cluster=rl_cluster,\n",
                "    reward_fns=[\n",
                "        match_format_exactly,\n",
                "        match_format_approximately,\n",
                "        check_answer,\n",
                "        check_numbers,\n",
                "    ],\n",
                "    grpo_config=grpo_config,\n",
                ")\n",
                "\n",
                "print(\"Starting training...\")\n",
                "with mesh:\n",
                "    grpo_trainer.train(train_dataset)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "kaggle": {
            "accelerator": "tpuV5e8",
            "isInternetEnabled": true,
            "isGpuEnabled": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}